{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3854ecc5-a9f9-4f38-afe6-8840f4335bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"people.json\")\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81183ef0-ff74-4d90-adfa-ad1b117da3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(filepath):\n",
    "    dictionary_of_interest = {}\n",
    "\n",
    "    with open(filepath,\"r\",encoding=\"utf8\") as file:\n",
    "            \n",
    "        content = file.readlines()\n",
    "        content = \"\".join(content)\n",
    "        \n",
    "        bs_content = bs(content, \"lxml\")\n",
    "\n",
    "        unique_id = bs_content.find(\"tei\").attrs[\"xml:id\"]\n",
    "\n",
    "        letter_details = bs_content.find_all(\"correspaction\")\n",
    "\n",
    "        for deets in letter_details:\n",
    "\n",
    "            if deets.attrs[\"type\"] == \"sent\":\n",
    "\n",
    "                try:\n",
    "                    dictionary_of_interest[\"unique_id\"] = unique_id\n",
    "                    dictionary_of_interest[\"sender\"] = deets.persname.text\n",
    "\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.orgname.text\n",
    "\n",
    "                if \"when\" in list(deets.date.attrs.keys()):\n",
    "                    dictionary_of_interest[\"date\"] = deets.date.attrs[\"when\"]\n",
    "\n",
    "                try:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = deets.persname.attrs[\"key\"]\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = \"None Available\"\n",
    "                except KeyError:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = \"None Available\"\n",
    "                    \n",
    "            if deets.attrs[\"type\"] == \"received\":\n",
    "                \n",
    "                try:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.persname.text\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.orgname.text\n",
    "                    \n",
    "                try:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = deets.persname.attrs[\"key\"]\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = \"None Available\"\n",
    "                except KeyError:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = \"None Available\"\n",
    "                    \n",
    "        try:\n",
    "            free_text = bs_content.find_all(\"div\",{\"type\":\"transcription\"})[0].p.text\n",
    "        except AttributeError:\n",
    "#             print(bs_content) \n",
    "            free_text = \"\"\n",
    "\n",
    "        # cleaning of the data\n",
    "        free_text = free_text.lower().translate(str.maketrans('','',string.punctuation))\n",
    "        go_away_chars = ['’', '“', '‘', '〈', '〉', '–', '♂', '…', '♀', '〈', '〉', '☿', '§', '⊙', '▵', '∴', '„', '✓']\n",
    "        for char in go_away_chars:\n",
    "            free_text = str.replace(free_text, char, \"_\")\n",
    "        dictionary_of_interest[\"body\"] = free_text\n",
    "\n",
    "        file.close()\n",
    "\n",
    "    return dictionary_of_interest\n",
    "\n",
    "def generate_feature_data(free_text,feature_set):\n",
    "    \n",
    "    feature_bools = []\n",
    "    \n",
    "    for word in feature_set:\n",
    "        feature_bools.append(1*(word in free_text))\n",
    "        \n",
    "    return feature_bools\n",
    "\n",
    "def convert_dictionary_to_dataset_for_gender(data_set,feature_words,sender):\n",
    "    \n",
    "    # test = generate_feature_data(data_set[0][\"body\"],feature_words)\n",
    "    complete_data = []\n",
    "    incomplete_data = []\n",
    "    targets = []\n",
    "    for dictionary in not_darwin_dict:\n",
    "        free_text = dictionary[\"body\"]\n",
    "        try:\n",
    "            person_id = dictionary[sender]\n",
    "        except:\n",
    "            print(dictionary)\n",
    "            continue\n",
    "        number_key = person_id[21:-4]\n",
    "\n",
    "        boolean_set = generate_feature_data(free_text,feature_words)\n",
    "        try:\n",
    "            dft = df[df[\"id\"]==\"DCP-IDENT-\"+str(number_key)]\n",
    "            gender = dft[\"occupation\"].iloc[0]\n",
    "    #         if dft[\"name\"].iloc[0] == \"John Jenner Weir\":\n",
    "    #             print(gender)\n",
    "            if gender == \"no common occupation\":\n",
    "                incomplete_data.append(boolean_set)\n",
    "                continue\n",
    "    #             gender = \"NotAvailable\"\n",
    "    #             print(number_key)\n",
    "        except:\n",
    "            continue\n",
    "    #         gender = \"NotAvailable\"\n",
    "    #         print(number_key)\n",
    "    #         print(reciever_id)\n",
    "    #         print(df[df[\"id\"]==\"DCP-IDENT-\"+str(number_key)])\n",
    "\n",
    "        complete_data.append(boolean_set)\n",
    "        targets.append(gender)\n",
    "        \n",
    "    return complete_data,incomplete_data,targets\n",
    "\n",
    "def file_to_features(filepath):\n",
    "    \n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    lines_cleaned = []\n",
    "    for word in lines:\n",
    "        word = word[:-1]\n",
    "        if word not in stopwords.words():\n",
    "            lines_cleaned.append(word)\n",
    "\n",
    "    feature_words = lines_cleaned\n",
    "    \n",
    "    f.close()\n",
    "    return feature_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd0ce93-37de-4312-9646-c94191aad63b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints DCP-LETT-9999.xml\n",
      "100.0\n",
      "Analysis Finished\n",
      "When all words are extracted, we have got a dataset of 593478 words in letters TO Darwin\n",
      "When all words are extracted, we have got a dataset of 695012 words in letters FROM Darwin\n"
     ]
    }
   ],
   "source": [
    "path = \"dcp-data/letters/\"\n",
    "files = os.listdir(path)\n",
    "print(files[0],files[-1])\n",
    "# to be commented out depending on who is running the code (lol) \n",
    "files = files[1:]\n",
    "words_darwin = []\n",
    "words_not_darwin = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "not_darwin_dict = []\n",
    "\n",
    "for file_target in files:\n",
    "    dict_cur = extract_info(path+file_target)\n",
    "    cur_words = words_not_darwin\n",
    "    if \"sender\" in dict_cur.keys() and dict_cur[\"sender\"] == \"Darwin, C. R.\":\n",
    "        cur_words = words_darwin\n",
    "    else:\n",
    "        not_darwin_dict.append(dict_cur)\n",
    "    text_tokens = word_tokenize(dict_cur[\"body\"])\n",
    "\n",
    "    for word in text_tokens:\n",
    "        if len(word) == 1 and not(word in [\"i\",\"a\"]):\n",
    "            continue\n",
    "        else:\n",
    "            cur_words.append(word)\n",
    "            \n",
    "    i += 1\n",
    "    print(round((i/len(files))*100,2),end=\"\\r\"*(i!=len(files)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Analysis Finished\")\n",
    "print(f\"When all words are extracted, we have got a dataset of {len(words_not_darwin)} words in letters TO Darwin\")\n",
    "print(f\"When all words are extracted, we have got a dataset of {len(words_darwin)} words in letters FROM Darwin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "435f83f9-0730-4e4c-996a-540786836506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if \"dump.txt\" not in os.listdir(\".\"):\n",
    "#     unique_words = {}\n",
    "#     for counter, word in enumerate(words):\n",
    "#         try:\n",
    "#             unique_words[word] += 1\n",
    "#         except KeyError:\n",
    "#             unique_words[word] = 1\n",
    "#         print(round(((counter+1)/len(words))*100,2),end=\"\\r\")\n",
    "\n",
    "#     sorted_unique_words = {key: value for key, value in sorted(unique_words.items(), key=lambda item: item[1],reverse=True)}\n",
    "\n",
    "# #     print(list(sorted_unique_words.keys())[:1000])\n",
    "#     feature_words_unclean = list(sorted_unique_words.keys())[:5000]\n",
    "#     feature_words = []\n",
    "#     for word in feature_words_unclean:\n",
    "#         if word not in stopwords.words():\n",
    "#             feature_words.append(word)\n",
    "            \n",
    "#     with open(\"dump.txt\",\"w\",encoding=\"utf8\") as output:\n",
    "#         for word in feature_words:\n",
    "#             try:\n",
    "#                 output.write(word +\"\\n\")\n",
    "#             except:\n",
    "#                 print(word)\n",
    "\n",
    "#     output.close()\n",
    "\n",
    "feature_words = file_to_features(\"dump_not_darwin.txt\")\n",
    "# print(sum(list(sorted_unique_words.values())[:4000]))\n",
    "# print(sum(list(sorted_unique_words.values())[4000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03d38b16-8d6c-4a28-a3b1-2564e0f8ac4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test = generate_feature_data(data_set[0][\"body\"],feature_words)\n",
    "complete_data,incomplete_data,targets = convert_dictionary_to_dataset_for_gender(not_darwin_dict,feature_words,\"sender_bio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7dba1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'publisher': 239, 'zoologist': 413, 'traveller': 103, 'botanist': 1306, 'philosopher': 25, 'comparative anatomist': 98, 'naturalist': 358, 'politician': 115, 'mathematician': 146, 'civil servant': 91, 'clergyman': 254, 'agricultural chemist': 10, 'journalist': 16, 'travel writer': 5, 'barrister': 54, 'entomologist': 179, 'printer': 1, 'horticulturist': 15, 'anatomist': 43, 'jurist': 13, 'lawyer': 34, 'surveyor': 21, 'teacher': 9, 'explorer': 24, 'architect': 4, 'businessman': 12, 'naval officer': 73, 'orientalist': 6, 'banker': 183, 'mining engineer': 20, 'physicist': 23, 'author': 185, 'land agent': 12, 'statesman': 17, 'geologist': 214, 'librarian': 6, 'bookseller': 12, 'mineralogist': 10, 'actor': 1, 'embryologist': 13, 'biologist': 3, 'editor': 68, 'gardener': 26, 'palaeontologist': 101, 'chemist': 64, 'nurseryman': 23, 'physiologist': 71, 'schoolteacher': 25, 'divine': 1, 'educationalist': 2, 'anthropologist': 21, 'accountant': 2, 'collector': 2, 'civil engineer': 28, 'poet': 8, 'psychologist': 9, 'army officer': 23, 'diplomat': 30, 'agriculturist': 1, 'illustrator': 6, 'engineer': 11, 'writer': 110, 'academic': 2, 'pharmacist': 60, 'military engineer': 15, 'inventor': 2, 'solicitor': 25, 'anglican clergyman': 9, 'administrator': 1, 'photographer': 11, 'socialist': 4, 'missionary': 9, 'merchant': 8, 'engraver': 1, 'educator': 7, 'social reformer': 6, 'manufacturer': 1, 'judge': 20, 'novelist': 11, 'man of letters': 6, 'astronomer': 3, 'farmer': 24, 'landowner': 9, 'marine zoologist': 4, 'reformer': 2, 'painter': 1, 'arctic explorer': 2, 'ornithologist': 12, 'natural historian': 4, 'artist': 9, 'magistrate': 4, 'curator': 4, 'agriculturalist': 7, 'industrialist': 2, 'landscape painter': 1, 'horticulturalist': 8, 'essayist': 6, 'whig politician': 1, 'archaeologist': 2, 'military officer': 1, 'scientific writer': 12, 'botanical collector': 1, 'bryologist': 1, 'philologist': 4, 'forester': 3, 'meteorologist': 2, 'sculptor': 3, 'historian': 2, 'surgeon': 2, 'statistician': 8, 'colonial administrator': 1, 'dentist': 2, 'sportsman': 9, 'agricultural writer': 1, 'conchologist': 1, 'translator': 7, 'head of state': 3, 'ethnologist': 1, 'physician': 5, 'educationist': 3, 'justice of the peace': 1, 'lexicographer': 2}\n",
      "0.24293154761904762\n"
     ]
    }
   ],
   "source": [
    "unique_tags = {}\n",
    "for val in targets:\n",
    "    try:\n",
    "        unique_tags[val] += 1\n",
    "    except:\n",
    "        unique_tags[val] = 1\n",
    "        \n",
    "print(unique_tags)\n",
    "\n",
    "sorted_unique_words = {key: value for key, value in sorted(unique_tags.items(), key=lambda item: item[1],reverse=True)}\n",
    "\n",
    "print(list(sorted_unique_words.values())[0]/sum(sorted_unique_words.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a01ce938-cb3e-4efd-8bb3-c7771599ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    complete_data, targets, test_size=0.4, random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ed2b2d2-d1a2-4025-8f0f-4d44356134e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1817759181775918\n",
      "0.9953488372093023\n"
     ]
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(5,weights=\"distance\")\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(score)\n",
    "score = classifier.score(X_train, y_train)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49b90e2b-466f-4ac9-a3b2-4c368ec4bd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6698288690476191\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.score(complete_data,targets)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b68c037-e8d3-4389-870c-efe09ffbada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(max_depth=10)\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f783590-81d4-4b96-8881-8deeca199ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "botanist\n",
      "author\n",
      "['academic' 'actor' 'agricultural chemist' 'agricultural writer'\n",
      " 'agriculturalist' 'anatomist' 'anglican clergyman' 'anthropologist'\n",
      " 'archaeologist' 'architect' 'arctic explorer' 'army officer' 'artist'\n",
      " 'astronomer' 'author' 'banker' 'barrister' 'biologist' 'bookseller'\n",
      " 'botanical collector' 'botanist' 'businessman' 'chemist' 'civil engineer'\n",
      " 'civil servant' 'clergyman' 'collector' 'colonial administrator'\n",
      " 'comparative anatomist' 'curator' 'dentist' 'diplomat' 'divine' 'editor'\n",
      " 'educationalist' 'educator' 'embryologist' 'engineer' 'engraver'\n",
      " 'entomologist' 'essayist' 'ethnologist' 'explorer' 'farmer' 'forester'\n",
      " 'gardener' 'geologist' 'head of state' 'historian' 'horticulturalist'\n",
      " 'horticulturist' 'illustrator' 'industrialist' 'inventor' 'journalist'\n",
      " 'judge' 'jurist' 'land agent' 'landowner' 'lawyer' 'lexicographer'\n",
      " 'librarian' 'magistrate' 'man of letters' 'manufacturer'\n",
      " 'marine zoologist' 'mathematician' 'merchant' 'meteorologist'\n",
      " 'military engineer' 'military officer' 'mineralogist' 'mining engineer'\n",
      " 'missionary' 'natural historian' 'naturalist' 'naval officer' 'novelist'\n",
      " 'nurseryman' 'orientalist' 'ornithologist' 'palaeontologist' 'pharmacist'\n",
      " 'philologist' 'philosopher' 'photographer' 'physician' 'physicist'\n",
      " 'physiologist' 'poet' 'politician' 'printer' 'psychologist' 'publisher'\n",
      " 'reformer' 'schoolteacher' 'scientific writer' 'sculptor'\n",
      " 'social reformer' 'socialist' 'solicitor' 'sportsman' 'statesman'\n",
      " 'statistician' 'surgeon' 'surveyor' 'teacher' 'translator'\n",
      " 'travel writer' 'traveller' 'whig politician' 'writer' 'zoologist']\n",
      "[0.23837701 0.31022823 0.37573964 0.42180896 0.4649197  0.50507185\n",
      " 0.54311074 0.57480981 0.60270499 0.62806424 0.65131023 0.6737109\n",
      " 0.69442096 0.71470837 0.73499577 0.75063398 0.76584954 0.78106509\n",
      " 0.79543533 0.80896027 0.81825866 0.8262891  0.83389687 0.83981403\n",
      " 0.84573119 0.8512257  0.85629755 0.8613694  0.86644125 0.8715131\n",
      " 0.8761623  0.8808115  0.88503804 0.88926458 0.89349112 0.89771767\n",
      " 0.90194421 0.90617075 0.90955199 0.91293322 0.91631445 0.91969569\n",
      " 0.92307692 0.92645816 0.92941674 0.93237532 0.93491124 0.93744717\n",
      " 0.93998309 0.94251902 0.94463229 0.94674556 0.94885883 0.9509721\n",
      " 0.95308538 0.95519865 0.95688926 0.95857988 0.9602705  0.96196112\n",
      " 0.96365173 0.96534235 0.96703297 0.96872358 0.9704142  0.97168216\n",
      " 0.97295013 0.97421809 0.97548605 0.97675402 0.97802198 0.97928994\n",
      " 0.9805579  0.98140321 0.98224852 0.98309383 0.98393914 0.98478445\n",
      " 0.98562975 0.98647506 0.98732037 0.98816568 0.98858833 0.98901099\n",
      " 0.98943364 0.9898563  0.99027895 0.99070161 0.99112426 0.99154691\n",
      " 0.99196957 0.99239222 0.99281488 0.99323753 0.99366019 0.99408284\n",
      " 0.99450549 0.99492815 0.9953508  0.99577346 0.99619611 0.99661877\n",
      " 0.99704142 0.99746407 0.99788673 0.99830938 0.99873204 0.99915469\n",
      " 0.99957735 1.         1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "classes = classifier.predict_proba(X_test)\n",
    "test = classifier.predict(X_test)\n",
    "print(test[0])\n",
    "print(y_test[0])\n",
    "types = classifier.classes_\n",
    "for row in classes:\n",
    "    sorted_row = sorted(row,reverse=True)\n",
    "    for val in sorted_row:\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ff5983d-622f-4be8-8a1b-adb8b3627016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = classifier.predict(incomplete_data)\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f54abc86-68d4-4323-a825-6fad9aad79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = classifier.predict(complete_data)\n",
    "# classifier.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afd31932-8b65-4e41-9fb3-046700fb7299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i,val in enumerate(out):\n",
    "#     if val != targets[i]:\n",
    "#         print(val,targets[i],i)\n",
    "#         print(not_darwin_dict[i][\"body\"])\n",
    "#         if val == \"M\" and targets[i] == \"F\":\n",
    "#             print(not_darwin_dict[i][\"sender\"])\n",
    "#             print(not_darwin_dict[i][\"sender_bio\"])\n",
    "#             tag = not_darwin_dict[i][\"sender_bio\"][21:-4]\n",
    "#             try:\n",
    "#                 print(df[df[\"id\"]==\"DCP-IDENT-\"+tag][\"sex\"].iloc[0])\n",
    "#                 print(df[df[\"id\"]==\"DCP-IDENT-\"+tag])\n",
    "#             except:\n",
    "#                 pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fdbe76f-9c28-4d2d-889a-894faf1e5d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter 0.00752737001051071\n",
      "think 0.005942660534613718\n",
      "may 0.007757319112782476\n",
      "two 0.008428306694029936\n",
      "kind 0.01231528100454832\n",
      "species 0.00809333768047392\n",
      "sent 0.013866207914098672\n",
      "return 0.008319724748459201\n",
      "whether 0.014277446917911331\n",
      "year 0.010403596691896678\n",
      "therefore 0.015285223215650344\n",
      "back 0.006338837903587963\n",
      "plant 0.058905973142512905\n",
      "look 0.010496127697499547\n",
      "form 0.0039617736897424784\n",
      "ought 0.0039617736897424784\n",
      "full 0.007329281326023582\n",
      "call 0.005942660534613718\n",
      "try 0.01382218820643487\n",
      "wd 0.021487506339042374\n",
      "copies 0.06282819577715223\n",
      "wh 0.025331424206437417\n",
      "accept 0.0039617736897424784\n",
      "month 0.0039617736897424784\n",
      "honour 0.007420870717775692\n",
      "cases 0.007801646650569803\n",
      "thinks 0.007363296554672886\n",
      "st 0.021391881190524504\n",
      "delay 0.0039617736897424784\n",
      "animal 0.016009013921739763\n",
      "ready 0.006602956149570798\n",
      "wishes 0.006678418505565892\n",
      "position 0.0039617736897424784\n",
      "rest 0.008715902117433456\n",
      "age 0.027140056114459476\n",
      "ms 0.011652275558066113\n",
      "museum 0.009642258024299705\n",
      "real 0.007043153226208852\n",
      "act 0.007131192641536461\n",
      "distance 0.013617199932622979\n",
      "insect 0.006600175957507808\n",
      "ordinary 0.0039617736897424784\n",
      "wants 0.0076594291335021205\n",
      "self 0.0039617736897424784\n",
      "flora 0.02298988647266104\n",
      "print 0.01687726489432343\n",
      "fit 0.008710822920395328\n",
      "coral 0.007843511547368951\n",
      "erhalten 0.026947738233770918\n",
      "freude 0.018110965438822756\n",
      "presented 0.0039617736897424784\n",
      "chief 0.0039617736897424784\n",
      "co 0.009846772941366223\n",
      "werkes 0.01754269072084241\n",
      "oxalis 0.0039617736897424784\n",
      "utricularia 0.006338837903587963\n",
      "wch 0.02194931800296086\n",
      "prospect 0.0039617736897424784\n",
      "lythrum 0.01484686883322519\n",
      "sun 0.01092903086825512\n",
      "unusual 0.0039617736897424784\n",
      "eben 0.015903592955751823\n",
      "ld 0.009684335686037165\n",
      "clowes 0.044451017573338136\n",
      "owe 0.009904434224356195\n",
      "awful 0.0212620123323549\n",
      "detail 0.0039617736897424784\n",
      "expense 0.007204677153523626\n",
      "sage 0.01323462080413973\n",
      "tend 0.0039617736897424784\n",
      "ape 0.008355740872911405\n",
      "glorious 0.0039617736897424784\n",
      "pu 0.007263251764527875\n",
      "sick 0.008283708624006994\n",
      "secure 0.006791612039558535\n",
      "ample 0.006338837903587963\n",
      "seedling 0.012264020024864477\n",
      "printer 0.0039617736897424784\n",
      "caird 0.03302526254861469\n",
      "tin 0.009508256855381947\n",
      "farrer 0.012217195139751712\n",
      "port 0.006933103957049336\n",
      "excepting 0.006338837903587963\n",
      "stellung 0.01358322407911707\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "# print(len(classifier.feature_importances_))\n",
    "\n",
    "test_words = []\n",
    "counter = 0\n",
    "for i,val in enumerate(classifier.feature_importances_):\n",
    "    if val != 0:\n",
    "        counter += 1\n",
    "        print(feature_words[i],val)\n",
    "        \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a622f989-ab92-4b56-aef6-c45c3614bd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24500232450023246\n",
      "0.25550387596899227\n"
     ]
    }
   ],
   "source": [
    "classifier = AdaBoostClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(score)\n",
    "score = classifier.score(X_train, y_train)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c932e1a2-9319-43be-8dbb-7d7d8c901c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copies 0.02\n",
      "beschäftigt 0.32\n",
      "honorary 0.32\n",
      "rostellum 0.34\n"
     ]
    }
   ],
   "source": [
    "for i,val in enumerate(classifier.feature_importances_):\n",
    "    if val != 0:\n",
    "        print(feature_words[i],val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3249664-2eeb-48d2-9f72-28b616f97458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i,dictionary in enumerate(not_darwin_dict):\n",
    "    \n",
    "    free_text = dictionary[\"body\"]\n",
    "    \n",
    "    try:\n",
    "        reciever_id = dictionary[\"reciever_bio\"]\n",
    "    except:\n",
    "        print(dictionary)\n",
    "        continue\n",
    "    number_key = reciever_id[21:-4]\n",
    "    \n",
    "    dft = df[df[\"id\"]==\"DCP-IDENT-\"+str(number_key)]\n",
    "    \n",
    "    try:\n",
    "        key_words = dft[\"keywords\"].iloc[0]\n",
    "    except:\n",
    "        key_words = \"None\"\n",
    "    print(key_words)\n",
    "    if i == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06f75a-4ec3-45bf-aede-99d152e28059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

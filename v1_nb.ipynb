{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3854ecc5-a9f9-4f38-afe6-8840f4335bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81183ef0-ff74-4d90-adfa-ad1b117da3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(filepath,words):\n",
    "    \n",
    "    dictionary_of_interest = {}\n",
    "    \n",
    "    with open(filepath,\"r\",encoding=\"utf8\") as file:\n",
    "            \n",
    "        content = file.readlines()\n",
    "        content = \"\".join(content)\n",
    "        \n",
    "        bs_content = bs(content, \"lxml\")\n",
    "        \n",
    "        unique_id = bs_content.find(\"tei\").attrs[\"xml:id\"]\n",
    "        \n",
    "        dictionary_of_interest[\"unique_id\"] = unique_id\n",
    "        \n",
    "        letter_details = bs_content.find_all(\"correspaction\")\n",
    "        \n",
    "        for deets in letter_details:\n",
    "            \n",
    "            if deets.attrs[\"type\"] == \"sent\":\n",
    "                \n",
    "                if \"when\" in list(deets.date.attrs.keys()):\n",
    "                    dictionary_of_interest[\"date\"] = deets.date.attrs[\"when\"]\n",
    "                \n",
    "                try:\n",
    "                    dictionary_of_interest[\"sender\"] = deets.persname.text\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.orgname.text\n",
    "                \n",
    "                try:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = deets.persname.attrs[\"key\"]\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = \"None Available\"\n",
    "                except KeyError:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = \"None Available\"\n",
    "                    \n",
    "            if deets.attrs[\"type\"] == \"received\":\n",
    "                \n",
    "                try:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.persname.text\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.orgname.text\n",
    "                    \n",
    "                try:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = deets.persname.attrs[\"key\"]\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = \"None Available\"\n",
    "                except KeyError:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = \"None Available\"\n",
    "                    \n",
    "        try:\n",
    "            free_text = bs_content.find_all(\"div\",{\"type\":\"transcription\"})[0].p.text\n",
    "        except AttributeError:\n",
    "#             print(bs_content) \n",
    "            free_text = \"\"\n",
    "            \n",
    "\n",
    "        # cleaning of the data\n",
    "        dictionary_of_interest[\"body\"] = (free_text.lower()).translate(str.maketrans('','',string.punctuation))\n",
    "        text_tokens = word_tokenize(dictionary_of_interest[\"body\"])\n",
    "#         no_stop_words = [word for word in text_tokens] # if not word in stopwords.words()\n",
    "        for word in text_tokens:\n",
    "            words.append(word)\n",
    "#         print(no_stop_words)\n",
    "\n",
    "        file.close()\n",
    "        \n",
    "    return dictionary_of_interest\n",
    "\n",
    "def generate_feature_data(free_text,feature_set):\n",
    "    \n",
    "    feature_bools = []\n",
    "    \n",
    "    for word in feature_set:\n",
    "        feature_bools.append(1*(word in free_text))\n",
    "        \n",
    "    return feature_bools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd0ce93-37de-4312-9646-c94191aad63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.88\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2318b1594226>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile_target\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdict_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m#     print(dict_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-cd914a8cd25e>\u001b[0m in \u001b[0;36mextract_info\u001b[1;34m(filepath, words)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mbs_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lxml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0munique_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs_content\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tei\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"xml:id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m         \u001b[1;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\builder\\_lxml.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParserError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._ExceptionContext._raise_if_stored\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/saxparser.pxi\u001b[0m in \u001b[0;36mlxml.etree._handleSaxTargetStartNoNs\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/saxparser.pxi\u001b[0m in \u001b[0;36mlxml.etree._callTargetSaxStart\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._PythonSaxParserTarget._handleSaxStart\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\builder\\_lxml.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self, name, attrs, nsmap)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnsmaps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT_NSMAPS_INVERTED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnsmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;31m# Make sure attrs is a mutable dict--lxml may send an immutable dictproxy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = \"dcp-data/letters/\"\n",
    "files = os.listdir(path)\n",
    "files = files[1:]\n",
    "words = []\n",
    "# print(files[1])\n",
    "i = 0\n",
    "cap = 0\n",
    "\n",
    "if cap == 0:\n",
    "    cap = len(files)\n",
    "    \n",
    "for file_target in files:\n",
    "    dict_test = extract_info(path+file_target,words)\n",
    "#     print(dict_test)\n",
    "    if i == cap:\n",
    "        break\n",
    "    elif i < cap:\n",
    "        i += 1\n",
    "    else:\n",
    "        print(\"Failed loop\")\n",
    "        break\n",
    "    print(round((i/cap)*100,2),end=\"\\r\"*(i!=cap))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Analysis Finished\")\n",
    "print(f\"When all words are extracted, we have got a dataset of {len(words)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b2cf6e6-0718-442e-b657-921b5463df7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unique_id': 'DCP-LETT-1095', 'date': '1847-06-10', 'sender': 'Darwin, C. R.', 'sender_bio': '../nameregs/nameregs_1.xml', 'reciever': 'Hooker, J. D.', 'reciever_bio': '../nameregs/nameregs_2357.xml', 'body': 'many thanks for your kindness about the lodgings—it will be of great use to me please let me know the address if mrs jacobson succeeds for i think i shall go on the 22d  write previously to my lodgings i have since had a tempting invitation from daubeny to meet henslow c but upon the whole i believe lodgings will answer best for then i shall have a secure solitary retreat to rest in—'}\n"
     ]
    }
   ],
   "source": [
    "# unique_words = {}\n",
    "# for counter, word in enumerate(words):\n",
    "#     try:\n",
    "#         unique_words[word] += 1\n",
    "#     except KeyError:\n",
    "#         unique_words[word] = 1\n",
    "#     print(round(((counter+1)/len(words))*100,2),end=\"\\r\")\n",
    "    \n",
    "# sorted_unique_words = {key: value for key, value in sorted(unique_words.items(), key=lambda item: item[1],reverse=True)}\n",
    "\n",
    "# print(list(sorted_unique_words.keys())[:1000])\n",
    "print(dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "435f83f9-0730-4e4c-996a-540786836506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_words = list(sorted_unique_words.keys())[:4000]\n",
    "# with open(\"dump.txt\",\"w\",encoding=\"utf8\") as output:\n",
    "#     for word in feature_words:\n",
    "#         try:\n",
    "#             output.write(word +\"\\n\")\n",
    "#         except:\n",
    "#             print(word)\n",
    "        \n",
    "# output.close()\n",
    "with open('dump.txt', encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "lines_cleaned = []\n",
    "for word in lines:\n",
    "    word = word[:-1]\n",
    "    lines_cleaned.append(word)\n",
    "    \n",
    "feature_words = lines_cleaned\n",
    "# print(sum(list(sorted_unique_words.values())[:4000]))\n",
    "# print(sum(list(sorted_unique_words.values())[4000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d38b16-8d6c-4a28-a3b1-2564e0f8ac4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = generate_feature_data(dict_test[\"body\"],feature_words)\n",
    "print(dict_test[\"sender\"])\n",
    "print(dict_test[\"reciever\"])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14fb45d-5371-4d9c-860f-827720b5083d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

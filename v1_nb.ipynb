{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854ecc5-a9f9-4f38-afe6-8840f4335bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81183ef0-ff74-4d90-adfa-ad1b117da3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(filepath,words):\n",
    "    \n",
    "    dictionary_of_interest = {}\n",
    "    \n",
    "    with open(filepath,\"r\",encoding=\"utf8\") as file:\n",
    "            \n",
    "        content = file.readlines()\n",
    "        content = \"\".join(content)\n",
    "        \n",
    "        bs_content = bs(content, \"lxml\")\n",
    "        \n",
    "        unique_id = bs_content.find(\"tei\").attrs[\"xml:id\"]\n",
    "        \n",
    "        dictionary_of_interest[\"unique_id\"] = unique_id\n",
    "        \n",
    "        letter_details = bs_content.find_all(\"correspaction\")\n",
    "        \n",
    "        for deets in letter_details:\n",
    "            \n",
    "            if deets.attrs[\"type\"] == \"sent\":\n",
    "                \n",
    "                if \"when\" in list(deets.date.attrs.keys()):\n",
    "                    dictionary_of_interest[\"date\"] = deets.date.attrs[\"when\"]\n",
    "                \n",
    "                try:\n",
    "                    dictionary_of_interest[\"sender\"] = deets.persname.text\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.orgname.text\n",
    "                \n",
    "                try:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = deets.persname.attrs[\"key\"]\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = \"None Available\"\n",
    "                except KeyError:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = \"None Available\"\n",
    "                    \n",
    "            if deets.attrs[\"type\"] == \"received\":\n",
    "                \n",
    "                try:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.persname.text\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.orgname.text\n",
    "                    \n",
    "                try:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = deets.persname.attrs[\"key\"]\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = \"None Available\"\n",
    "                except KeyError:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = \"None Available\"\n",
    "                    \n",
    "        try:\n",
    "            free_text = bs_content.find_all(\"div\",{\"type\":\"transcription\"})[0].p.text\n",
    "        except AttributeError:\n",
    "#             print(bs_content) \n",
    "            free_text = \"\"\n",
    "            \n",
    "\n",
    "        # cleaning of the data\n",
    "        dictionary_of_interest[\"body\"] = (free_text.lower()).translate(str.maketrans('','',string.punctuation))\n",
    "        text_tokens = word_tokenize(dictionary_of_interest[\"body\"])\n",
    "#         no_stop_words = [word for word in text_tokens] # if not word in stopwords.words()\n",
    "        for word in text_tokens:\n",
    "            words.append(word)\n",
    "#         print(no_stop_words)\n",
    "\n",
    "        file.close()\n",
    "        \n",
    "    return dictionary_of_interest\n",
    "\n",
    "def generate_feature_data(free_text,feature_set):\n",
    "    \n",
    "    feature_bools = []\n",
    "    \n",
    "    for word in feature_set:\n",
    "        feature_bools.append(1*(word in free_text))\n",
    "        \n",
    "    return feature_bools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd0ce93-37de-4312-9646-c94191aad63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dcp-data/letters/\"\n",
    "files = os.listdir(path)\n",
    "files = files[1:]\n",
    "words = []\n",
    "# print(files[1])\n",
    "i = 0\n",
    "cap = 10000\n",
    "\n",
    "if cap == 0:\n",
    "    cap = len(files)\n",
    "    \n",
    "for file_target in files:\n",
    "    dict_test = extract_info(path+file_target,words)\n",
    "#     print(dict_test)\n",
    "    if i == cap:\n",
    "        break\n",
    "    elif i < cap:\n",
    "        i += 1\n",
    "    else:\n",
    "        print(\"Failed loop\")\n",
    "        break\n",
    "    print((i/cap)*100,end=\"\\r\"*(i!=cap))\n",
    "\n",
    "print(\"Analysis Finished\")\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2cf6e6-0718-442e-b657-921b5463df7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_words = {}\n",
    "for counter, word in enumerate(words):\n",
    "    if word not in list(unique_words.keys()):\n",
    "        unique_words[word] = 1\n",
    "    else:\n",
    "        unique_words[word] += 1\n",
    "    print(round(((counter+1)/len(words))*100,2),end=\"\\r\")\n",
    "    \n",
    "sorted_unique_words = {key: value for key, value in sorted(unique_words.items(), key=lambda item: item[1],reverse=True)}\n",
    "\n",
    "# print(list(sorted_unique_words.keys())[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f83f9-0730-4e4c-996a-540786836506",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_words = list(sorted_unique_words.keys())[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d38b16-8d6c-4a28-a3b1-2564e0f8ac4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = generate_feature_data(dict_test[\"body\"],feature_words)\n",
    "print(dict_test[\"sender\"])\n",
    "print(dict_test[\"reciever\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

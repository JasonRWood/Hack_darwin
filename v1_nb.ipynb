{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3854ecc5-a9f9-4f38-afe6-8840f4335bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"people.json\")\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81183ef0-ff74-4d90-adfa-ad1b117da3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(filepath):\n",
    "    dictionary_of_interest = {}\n",
    "\n",
    "    with open(filepath,\"r\",encoding=\"utf8\") as file:\n",
    "            \n",
    "        content = file.readlines()\n",
    "        content = \"\".join(content)\n",
    "        \n",
    "        bs_content = bs(content, \"lxml\")\n",
    "\n",
    "        unique_id = bs_content.find(\"tei\").attrs[\"xml:id\"]\n",
    "\n",
    "        letter_details = bs_content.find_all(\"correspaction\")\n",
    "\n",
    "        for deets in letter_details:\n",
    "\n",
    "            if deets.attrs[\"type\"] == \"sent\":\n",
    "\n",
    "                try:\n",
    "                    dictionary_of_interest[\"unique_id\"] = unique_id\n",
    "                    dictionary_of_interest[\"sender\"] = deets.persname.text\n",
    "\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.orgname.text\n",
    "\n",
    "                if \"when\" in list(deets.date.attrs.keys()):\n",
    "                    dictionary_of_interest[\"date\"] = deets.date.attrs[\"when\"]\n",
    "\n",
    "                try:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = deets.persname.attrs[\"key\"]\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = \"None Available\"\n",
    "                except KeyError:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = \"None Available\"\n",
    "                    \n",
    "            if deets.attrs[\"type\"] == \"received\":\n",
    "                \n",
    "                try:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.persname.text\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.orgname.text\n",
    "                    \n",
    "                try:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = deets.persname.attrs[\"key\"]\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = \"None Available\"\n",
    "                except KeyError:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = \"None Available\"\n",
    "                    \n",
    "        try:\n",
    "            free_text = bs_content.find_all(\"div\",{\"type\":\"transcription\"})[0].p.text\n",
    "        except AttributeError:\n",
    "#             print(bs_content) \n",
    "            free_text = \"\"\n",
    "\n",
    "        # cleaning of the data\n",
    "        free_text = free_text.lower().translate(str.maketrans('','',string.punctuation))\n",
    "        go_away_chars = ['’', '“', '‘', '〈', '〉', '–', '♂', '…', '♀', '〈', '〉', '☿', '§', '⊙', '▵', '∴', '„', '✓']\n",
    "        for char in go_away_chars:\n",
    "            free_text = str.replace(free_text, char, \"_\")\n",
    "        dictionary_of_interest[\"body\"] = free_text\n",
    "\n",
    "        file.close()\n",
    "\n",
    "    return dictionary_of_interest\n",
    "\n",
    "def generate_feature_data(free_text,feature_set):\n",
    "    \n",
    "    feature_bools = []\n",
    "    \n",
    "    for word in feature_set:\n",
    "        feature_bools.append(1*(word in free_text))\n",
    "        \n",
    "    return feature_bools\n",
    "\n",
    "def convert_dictionary_to_dataset_for_gender(data_set,feature_words,sender):\n",
    "    \n",
    "    # test = generate_feature_data(data_set[0][\"body\"],feature_words)\n",
    "    complete_data = []\n",
    "    incomplete_data = []\n",
    "    targets = []\n",
    "    for dictionary in not_darwin_dict:\n",
    "        free_text = dictionary[\"body\"]\n",
    "        try:\n",
    "            person_id = dictionary[sender]\n",
    "        except:\n",
    "            print(dictionary)\n",
    "            continue\n",
    "        number_key = person_id[21:-4]\n",
    "\n",
    "        boolean_set = generate_feature_data(free_text,feature_words)\n",
    "        try:\n",
    "            dft = df[df[\"id\"]==\"DCP-IDENT-\"+str(number_key)]\n",
    "            gender = dft[\"sex\"].iloc[0]\n",
    "    #         if dft[\"name\"].iloc[0] == \"John Jenner Weir\":\n",
    "    #             print(gender)\n",
    "            if gender == \"\":\n",
    "                incomplete_data.append(boolean_set)\n",
    "                continue\n",
    "    #             gender = \"NotAvailable\"\n",
    "    #             print(number_key)\n",
    "        except:\n",
    "            continue\n",
    "    #         gender = \"NotAvailable\"\n",
    "    #         print(number_key)\n",
    "    #         print(reciever_id)\n",
    "    #         print(df[df[\"id\"]==\"DCP-IDENT-\"+str(number_key)])\n",
    "\n",
    "        complete_data.append(boolean_set)\n",
    "        targets.append(gender)\n",
    "        \n",
    "    return complete_data,incomplete_data,targets\n",
    "\n",
    "def file_to_features(filepath):\n",
    "    \n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    lines_cleaned = []\n",
    "    for word in lines:\n",
    "        word = word[:-1]\n",
    "        if word not in stopwords.words():\n",
    "            lines_cleaned.append(word)\n",
    "\n",
    "    feature_words = lines_cleaned\n",
    "    \n",
    "    f.close()\n",
    "    return feature_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd0ce93-37de-4312-9646-c94191aad63b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints DCP-LETT-9999.xml\n",
      "100.0\n",
      "Analysis Finished\n",
      "When all words are extracted, we have got a dataset of 593478 words in letters TO Darwin\n",
      "When all words are extracted, we have got a dataset of 695012 words in letters FROM Darwin\n"
     ]
    }
   ],
   "source": [
    "path = \"dcp-data/letters/\"\n",
    "files = os.listdir(path)\n",
    "print(files[0],files[-1])\n",
    "# to be commented out depending on who is running the code (lol) \n",
    "files = files[1:]\n",
    "words_darwin = []\n",
    "words_not_darwin = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "not_darwin_dict = []\n",
    "\n",
    "for file_target in files:\n",
    "    dict_cur = extract_info(path+file_target)\n",
    "    cur_words = words_not_darwin\n",
    "    if \"sender\" in dict_cur.keys() and dict_cur[\"sender\"] == \"Darwin, C. R.\":\n",
    "        cur_words = words_darwin\n",
    "    else:\n",
    "        not_darwin_dict.append(dict_cur)\n",
    "    text_tokens = word_tokenize(dict_cur[\"body\"])\n",
    "\n",
    "    for word in text_tokens:\n",
    "        if len(word) == 1 and not(word in [\"i\",\"a\"]):\n",
    "            continue\n",
    "        else:\n",
    "            cur_words.append(word)\n",
    "            \n",
    "    i += 1\n",
    "    print(round((i/len(files))*100,2),end=\"\\r\"*(i!=len(files)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Analysis Finished\")\n",
    "print(f\"When all words are extracted, we have got a dataset of {len(words_not_darwin)} words in letters TO Darwin\")\n",
    "print(f\"When all words are extracted, we have got a dataset of {len(words_darwin)} words in letters FROM Darwin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "435f83f9-0730-4e4c-996a-540786836506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if \"dump.txt\" not in os.listdir(\".\"):\n",
    "#     unique_words = {}\n",
    "#     for counter, word in enumerate(words):\n",
    "#         try:\n",
    "#             unique_words[word] += 1\n",
    "#         except KeyError:\n",
    "#             unique_words[word] = 1\n",
    "#         print(round(((counter+1)/len(words))*100,2),end=\"\\r\")\n",
    "\n",
    "#     sorted_unique_words = {key: value for key, value in sorted(unique_words.items(), key=lambda item: item[1],reverse=True)}\n",
    "\n",
    "# #     print(list(sorted_unique_words.keys())[:1000])\n",
    "#     feature_words_unclean = list(sorted_unique_words.keys())[:5000]\n",
    "#     feature_words = []\n",
    "#     for word in feature_words_unclean:\n",
    "#         if word not in stopwords.words():\n",
    "#             feature_words.append(word)\n",
    "            \n",
    "#     with open(\"dump.txt\",\"w\",encoding=\"utf8\") as output:\n",
    "#         for word in feature_words:\n",
    "#             try:\n",
    "#                 output.write(word +\"\\n\")\n",
    "#             except:\n",
    "#                 print(word)\n",
    "\n",
    "#     output.close()\n",
    "\n",
    "feature_words = file_to_features(\"dump_not_darwin.txt\")\n",
    "# print(sum(list(sorted_unique_words.values())[:4000]))\n",
    "# print(sum(list(sorted_unique_words.values())[4000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d38b16-8d6c-4a28-a3b1-2564e0f8ac4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test = generate_feature_data(data_set[0][\"body\"],feature_words)\n",
    "complete_data,incomplete_data,targets = convert_dictionary_to_dataset_for_gender(not_darwin_dict,feature_words,\"sender_bio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7dba1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F': 447, 'M': 6471}\n",
      "0.06461405030355594\n"
     ]
    }
   ],
   "source": [
    "unique_tags = {}\n",
    "for val in targets:\n",
    "    try:\n",
    "        unique_tags[val] += 1\n",
    "    except:\n",
    "        unique_tags[val] = 1\n",
    "        \n",
    "print(unique_tags)\n",
    "print(list(unique_tags.values())[0]/sum(unique_tags.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a01ce938-cb3e-4efd-8bb3-c7771599ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be opened for both files\n",
    "filenames = {\"darwin\": \"dump_darwin.txt\", \"not_darwin\": \"dump_not_darwin.txt\"}\n",
    "feature_words_all = []\n",
    "for key in filenames.keys():\n",
    "    file = filenames[key]\n",
    "    with open(file, \"w\", encoding=\"utf8\") as output:\n",
    "        for word in unique_both[key]:\n",
    "            try:\n",
    "                output.write(word +\"\\n\")\n",
    "            except:\n",
    "                print(word)\n",
    "        \n",
    "    output.close()\n",
    "\n",
    "    with open(file, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    lines_cleaned = []\n",
    "    for word in lines:\n",
    "        word = word[:-1]\n",
    "        lines_cleaned.append(word)\n",
    "    \n",
    "    feature_words = lines_cleaned\n",
    "    feature_words_all.append(feature_words)\n",
    "\n",
    "# print(sum(list(sorted_unique_words.values())[:4000]))\n",
    "# print(sum(list(sorted_unique_words.values())[4000:]))"
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    complete_data, targets, test_size=0.2, random_state=42, stratify=targets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ed2b2d2-d1a2-4025-8f0f-4d44356134e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9356936416184971\n",
      "0.999819298879653\n"
     ]
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(5,weights=\"distance\")\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(score)\n",
    "score = classifier.score(X_train, y_train)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49b90e2b-466f-4ac9-a3b2-4c368ec4bd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.986990459670425\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.score(complete_data,targets)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b68c037-e8d3-4389-870c-efe09ffbada3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9335260115606936\n"
     ]
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(max_depth=10)\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ff5983d-622f-4be8-8a1b-adb8b3627016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = classifier.predict(incomplete_data)\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f54abc86-68d4-4323-a825-6fad9aad79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = classifier.predict(complete_data)\n",
    "# classifier.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afd31932-8b65-4e41-9fb3-046700fb7299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i,val in enumerate(out):\n",
    "#     if val != targets[i]:\n",
    "#         print(val,targets[i],i)\n",
    "#         print(not_darwin_dict[i][\"body\"])\n",
    "#         if val == \"M\" and targets[i] == \"F\":\n",
    "#             print(not_darwin_dict[i][\"sender\"])\n",
    "#             print(not_darwin_dict[i][\"sender_bio\"])\n",
    "#             tag = not_darwin_dict[i][\"sender_bio\"][21:-4]\n",
    "#             try:\n",
    "#                 print(df[df[\"id\"]==\"DCP-IDENT-\"+tag][\"sex\"].iloc[0])\n",
    "#                 print(df[df[\"id\"]==\"DCP-IDENT-\"+tag])\n",
    "#             except:\n",
    "#                 pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fdbe76f-9c28-4d2d-889a-894faf1e5d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hope 0.005282264520355472\n",
      "make 0.025428525102344817\n",
      "thank 0.022846332743014747\n",
      "us 0.006811078241631451\n",
      "darwin 0.046013451328453116\n",
      "tell 0.023094900843258763\n",
      "still 0.020650527374378332\n",
      "best 0.008609573299108806\n",
      "soon 0.016008425353030445\n",
      "mrs 0.0071874562809524445\n",
      "plant 0.008731868374380246\n",
      "several 0.01907773654563757\n",
      "wrote 0.009580720182554575\n",
      "dear 0.01115004214845702\n",
      "charles 0.05016351892430722\n",
      "try 0.006272689117922131\n",
      "wife 0.005880646048052\n",
      "looked 0.010201485034351655\n",
      "containing 0.007603259536875308\n",
      "papa 0.2062315367366242\n",
      "fanny 0.022707676577016018\n",
      "answering 0.007527226941506558\n",
      "unwell 0.03863581134667083\n",
      "loss 0.007193679186823402\n",
      "sister 0.01089311113837545\n",
      "shew 0.005227240931601778\n",
      "member 0.005973989636116318\n",
      "comfort 0.09335556122364977\n",
      "fixed 0.007616836786048304\n",
      "ere 0.011667948508039682\n",
      "bitte 0.01338173678490055\n",
      "desires 0.02367443186011548\n",
      "catherine 0.014142967888532941\n",
      "husband 0.027666137833855714\n",
      "total 0.006969654575469036\n",
      "sweet 0.0200821814125486\n",
      "wings 0.005227240931601778\n",
      "jessie 0.02877804563279392\n",
      "joseph 0.02772364273675188\n",
      "feels 0.019514631275690336\n",
      "ran 0.013090829463489677\n",
      "leads 0.006860753722727333\n",
      "pp 0.01568172279480533\n",
      "lychnis 0.04958090307517894\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "# print(len(classifier.feature_importances_))\n",
    "\n",
    "test_words = []\n",
    "counter = 0\n",
    "for i,val in enumerate(classifier.feature_importances_):\n",
    "    if val != 0:\n",
    "        counter += 1\n",
    "        print(feature_words[i],val)\n",
    "        \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a622f989-ab92-4b56-aef6-c45c3614bd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9378612716763006\n",
      "0.9513913986266714\n"
     ]
    }
   ],
   "source": [
    "classifier = AdaBoostClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(score)\n",
    "score = classifier.score(X_train, y_train)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c932e1a2-9319-43be-8dbb-7d7d8c901c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "much 0.02\n",
      "mr 0.02\n",
      "must 0.02\n",
      "species 0.02\n",
      "read 0.02\n",
      "however 0.02\n",
      "copy 0.02\n",
      "subject 0.02\n",
      "got 0.02\n",
      "done 0.02\n",
      "sure 0.02\n",
      "came 0.02\n",
      "therefore 0.02\n",
      "dear 0.02\n",
      "father 0.02\n",
      "charles 0.02\n",
      "thus 0.02\n",
      "wd 0.02\n",
      "result 0.02\n",
      "coming 0.02\n",
      "room 0.02\n",
      "desire 0.02\n",
      "publish 0.02\n",
      "lecture 0.02\n",
      "real 0.02\n",
      "papa 0.02\n",
      "land 0.02\n",
      "act 0.02\n",
      "caroline 0.02\n",
      "fair 0.02\n",
      "begs 0.02\n",
      "sum 0.02\n",
      "bit 0.02\n",
      "mouth 0.02\n",
      "comfort 0.02\n",
      "proud 0.02\n",
      "desires 0.02\n",
      "play 0.02\n",
      "organ 0.02\n",
      "utricularia 0.02\n",
      "distinctly 0.02\n",
      "treat 0.02\n",
      "husband 0.02\n",
      "king 0.02\n",
      "select 0.02\n",
      "serve 0.02\n",
      "bloom 0.02\n",
      "joseph 0.02\n",
      "—but 0.02\n",
      "lychnis 0.02\n"
     ]
    }
   ],
   "source": [
    "for i,val in enumerate(classifier.feature_importances_):\n",
    "    if val != 0:\n",
    "        print(feature_words[i],val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3249664-2eeb-48d2-9f72-28b616f97458",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "None\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "None\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['nationality', 'Birmingham'], ['h', 'surgeon'], ['h', 'gynaecologist']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['name', 'Darwin, C. R.'], ['c', 'Publisher'], ['l', 'author of guidebooks']]\n",
      "[['name', 'Hordern, Peter'], ['name', 'clergyman'], ['name', 'Lubbock, John'], ['place', 'Chorlton-cum-Hardy'], ['place', 'Lancashire']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n",
      "[['occupation', 'naturalist'], ['occupation', 'author']]\n"
     ]
    }
   ],
   "source": [
    "for i,dictionary in enumerate(not_darwin_dict):\n",
    "    \n",
    "    free_text = dictionary[\"body\"]\n",
    "    \n",
    "    try:\n",
    "        reciever_id = dictionary[\"reciever_bio\"]\n",
    "    except:\n",
    "        print(dictionary)\n",
    "        continue\n",
    "    number_key = reciever_id[21:-4]\n",
    "    \n",
    "    dft = df[df[\"id\"]==\"DCP-IDENT-\"+str(number_key)]\n",
    "    \n",
    "    try:\n",
    "        key_words = dft[\"keywords\"].iloc[0]\n",
    "    except:\n",
    "        key_words = \"None\"\n",
    "    print(key_words)\n",
    "    if i == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06f75a-4ec3-45bf-aede-99d152e28059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
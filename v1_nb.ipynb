{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3854ecc5-a9f9-4f38-afe6-8840f4335bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"people.json\")\n",
    "\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81183ef0-ff74-4d90-adfa-ad1b117da3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(filepath,words):\n",
    "    \n",
    "    dictionary_of_interest = {}\n",
    "    \n",
    "    with open(filepath,\"r\",encoding=\"utf8\") as file:\n",
    "            \n",
    "        content = file.readlines()\n",
    "        content = \"\".join(content)\n",
    "        \n",
    "        bs_content = bs(content, \"lxml\")\n",
    "        \n",
    "        unique_id = bs_content.find(\"tei\").attrs[\"xml:id\"]\n",
    "        \n",
    "        dictionary_of_interest[\"unique_id\"] = unique_id\n",
    "        \n",
    "        letter_details = bs_content.find_all(\"correspaction\")\n",
    "        \n",
    "        for deets in letter_details:\n",
    "            \n",
    "            if deets.attrs[\"type\"] == \"sent\":\n",
    "                \n",
    "                if \"when\" in list(deets.date.attrs.keys()):\n",
    "                    dictionary_of_interest[\"date\"] = deets.date.attrs[\"when\"]\n",
    "                \n",
    "                try:\n",
    "                    dictionary_of_interest[\"sender\"] = deets.persname.text\n",
    "                    if dictionary_of_interest[\"sender\"] != \"Darwin, C. R.\":\n",
    "                        file.close()\n",
    "                        return \"Not Darwin Writing\"\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.orgname.text\n",
    "                \n",
    "                try:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = deets.persname.attrs[\"key\"]\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = \"None Available\"\n",
    "                except KeyError:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = \"None Available\"\n",
    "                    \n",
    "            if deets.attrs[\"type\"] == \"received\":\n",
    "                \n",
    "                try:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.persname.text\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.orgname.text\n",
    "                    \n",
    "                try:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = deets.persname.attrs[\"key\"]\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = \"None Available\"\n",
    "                except KeyError:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = \"None Available\"\n",
    "                    \n",
    "        try:\n",
    "            free_text = bs_content.find_all(\"div\",{\"type\":\"transcription\"})[0].p.text\n",
    "        except AttributeError:\n",
    "#             print(bs_content) \n",
    "            free_text = \"\"\n",
    "            \n",
    "\n",
    "        # cleaning of the data\n",
    "        dictionary_of_interest[\"body\"] = (free_text.lower()).translate(str.maketrans('','',string.punctuation))\n",
    "        text_tokens = word_tokenize(dictionary_of_interest[\"body\"])\n",
    "#         no_stop_words = [word for word in text_tokens] # if not word in stopwords.words()\n",
    "        for word in text_tokens:\n",
    "            words.append(word)\n",
    "#         print(no_stop_words)!\n",
    "\n",
    "        file.close()\n",
    "        \n",
    "    return dictionary_of_interest\n",
    "\n",
    "def generate_feature_data(free_text,feature_set):\n",
    "    \n",
    "    feature_bools = []\n",
    "    \n",
    "    for word in feature_set:\n",
    "        feature_bools.append(1*(word in free_text))\n",
    "        \n",
    "    return feature_bools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fd0ce93-37de-4312-9646-c94191aad63b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "Analysis Finished\n",
      "When all words are extracted, we have got a dataset of 705858 words\n"
     ]
    }
   ],
   "source": [
    "path = \"dcp-data/letters/\"\n",
    "files = os.listdir(path)\n",
    "files = files[1:]\n",
    "words = []\n",
    "# print(files[1])\n",
    "i = 0\n",
    "cap = 0\n",
    "\n",
    "if cap == 0:\n",
    "    cap = len(files)\n",
    "    \n",
    "data_set = []\n",
    "for file_target in files:\n",
    "    dict_test = extract_info(path+file_target,words)\n",
    "    if dict_test != \"Not Darwin Writing\":\n",
    "        data_set.append(dict_test)\n",
    "#     print(dict_test)\n",
    "    if i == cap:\n",
    "        break\n",
    "    elif i < cap:\n",
    "        i += 1\n",
    "    else:\n",
    "        print(\"Failed loop\")\n",
    "        break\n",
    "    print(round((i/cap)*100,2),end=\"\\r\"*(i!=cap))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Analysis Finished\")\n",
    "print(f\"When all words are extracted, we have got a dataset of {len(words)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "435f83f9-0730-4e4c-996a-540786836506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if \"dump.txt\" not in os.listdir(\".\"):\n",
    "    unique_words = {}\n",
    "    for counter, word in enumerate(words):\n",
    "        try:\n",
    "            unique_words[word] += 1\n",
    "        except KeyError:\n",
    "            unique_words[word] = 1\n",
    "        print(round(((counter+1)/len(words))*100,2),end=\"\\r\")\n",
    "\n",
    "    sorted_unique_words = {key: value for key, value in sorted(unique_words.items(), key=lambda item: item[1],reverse=True)}\n",
    "\n",
    "    print(list(sorted_unique_words.keys())[:1000])\n",
    "    feature_words = list(sorted_unique_words.keys())[:4000]\n",
    "    with open(\"dump.txt\",\"w\",encoding=\"utf8\") as output:\n",
    "        for word in feature_words:\n",
    "            try:\n",
    "                output.write(word +\"\\n\")\n",
    "            except:\n",
    "                print(word)\n",
    "\n",
    "    output.close()\n",
    "\n",
    "with open('dump.txt', encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "lines_cleaned = []\n",
    "for word in lines:\n",
    "    word = word[:-1]\n",
    "    lines_cleaned.append(word)\n",
    "    \n",
    "feature_words = lines_cleaned\n",
    "# print(sum(list(sorted_unique_words.values())[:4000]))\n",
    "# print(sum(list(sorted_unique_words.values())[4000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03d38b16-8d6c-4a28-a3b1-2564e0f8ac4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "None Available\n",
      "Empty DataFrame\n",
      "Columns: [id, sex, name, keywords]\n",
      "Index: []\n",
      "\n",
      "None Available\n",
      "Empty DataFrame\n",
      "Columns: [id, sex, name, keywords]\n",
      "Index: []\n",
      "\n",
      "None Available\n",
      "Empty DataFrame\n",
      "Columns: [id, sex, name, keywords]\n",
      "Index: []\n",
      "\n",
      "None Available\n",
      "Empty DataFrame\n",
      "Columns: [id, sex, name, keywords]\n",
      "Index: []\n",
      "\n",
      "None Available\n",
      "Empty DataFrame\n",
      "Columns: [id, sex, name, keywords]\n",
      "Index: []\n",
      "\n",
      "None Available\n",
      "Empty DataFrame\n",
      "Columns: [id, sex, name, keywords]\n",
      "Index: []\n",
      "\n",
      "None Available\n",
      "Empty DataFrame\n",
      "Columns: [id, sex, name, keywords]\n",
      "Index: []\n",
      "\n",
      "None Available\n",
      "Empty DataFrame\n",
      "Columns: [id, sex, name, keywords]\n",
      "Index: []\n",
      "\n",
      "None Available\n",
      "Empty DataFrame\n",
      "Columns: [id, sex, name, keywords]\n",
      "Index: []\n",
      "\n",
      "None Available\n",
      "Empty DataFrame\n",
      "Columns: [id, sex, name, keywords]\n",
      "Index: []\n",
      "\n",
      "None Available\n",
      "Empty DataFrame\n",
      "Columns: [id, sex, name, keywords]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "test = generate_feature_data(data_set[0][\"body\"],feature_words)\n",
    "\n",
    "complete_data = []\n",
    "targets = []\n",
    "for dictionary in data_set:\n",
    "    free_text = dictionary[\"body\"]\n",
    "    sender_id = dictionary[\"sender_bio\"]\n",
    "    number_key = sender_id[21:-4]\n",
    "    \n",
    "    boolean_set = generate_feature_data(free_text,feature_words)\n",
    "    try:\n",
    "        gender = df[df[\"id\"]==\"DCP-IDENT-\"+str(number_key)][\"sex\"].iloc[0]\n",
    "    except:\n",
    "        gender = \"NotAvailable\"\n",
    "        print(number_key)\n",
    "        print(sender_id)\n",
    "        print(df[df[\"id\"]==\"DCP-IDENT-\"+str(number_key)])\n",
    "        \n",
    "    complete_data.append(boolean_set)\n",
    "    targets.append(gender)\n",
    "# print(dict_test[\"sender\"])\n",
    "# print(dict_test[\"reciever\"])\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f14fb45d-5371-4d9c-860f-827720b5083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"F\" in targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a01ce938-cb3e-4efd-8bb3-c7771599ab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' '' 'F']\n"
     ]
    }
   ],
   "source": [
    "print(df.sex.unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

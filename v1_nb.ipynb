{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3854ecc5-a9f9-4f38-afe6-8840f4335bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"people.json\")\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81183ef0-ff74-4d90-adfa-ad1b117da3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(filepath,words):\n",
    "    \n",
    "    dictionary_of_interest = {}\n",
    "    \n",
    "    with open(filepath,\"r\",encoding=\"utf8\") as file:\n",
    "            \n",
    "        content = file.readlines()\n",
    "        content = \"\".join(content)\n",
    "        \n",
    "        bs_content = bs(content, \"lxml\")\n",
    "        \n",
    "        unique_id = bs_content.find(\"tei\").attrs[\"xml:id\"]\n",
    "        \n",
    "        dictionary_of_interest[\"unique_id\"] = unique_id\n",
    "        \n",
    "        letter_details = bs_content.find_all(\"correspaction\")\n",
    "        \n",
    "        for deets in letter_details:\n",
    "            \n",
    "            if deets.attrs[\"type\"] == \"sent\":\n",
    "                \n",
    "                if \"when\" in list(deets.date.attrs.keys()):\n",
    "                    dictionary_of_interest[\"date\"] = deets.date.attrs[\"when\"]\n",
    "                \n",
    "                try:\n",
    "                    dictionary_of_interest[\"sender\"] = deets.persname.text\n",
    "                    if dictionary_of_interest[\"sender\"] != \"Darwin, C. R.\":\n",
    "                        file.close()\n",
    "                        return \"Not Darwin Writing\"\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.orgname.text\n",
    "                \n",
    "                try:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = deets.persname.attrs[\"key\"]\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = \"None Available\"\n",
    "                except KeyError:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = \"None Available\"\n",
    "                    \n",
    "            if deets.attrs[\"type\"] == \"received\":\n",
    "                \n",
    "                try:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.persname.text\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.orgname.text\n",
    "                    \n",
    "                try:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = deets.persname.attrs[\"key\"]\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = \"None Available\"\n",
    "                except KeyError:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = \"None Available\"\n",
    "                    \n",
    "        try:\n",
    "            free_text = bs_content.find_all(\"div\",{\"type\":\"transcription\"})[0].p.text\n",
    "        except AttributeError:\n",
    "#             print(bs_content) \n",
    "            free_text = \"\"\n",
    "            \n",
    "\n",
    "        # cleaning of the data\n",
    "        dictionary_of_interest[\"body\"] = (free_text.lower()).translate(str.maketrans('','',string.punctuation))\n",
    "        text_tokens = word_tokenize(dictionary_of_interest[\"body\"])\n",
    "#         no_stop_words = [word for word in text_tokens] # if not word in stopwords.words()\n",
    "        for word in text_tokens:\n",
    "            words.append(word)\n",
    "#         print(no_stop_words)!\n",
    "\n",
    "        file.close()\n",
    "        \n",
    "    return dictionary_of_interest\n",
    "\n",
    "def generate_feature_data(free_text,feature_set):\n",
    "    \n",
    "    feature_bools = []\n",
    "    \n",
    "    for word in feature_set:\n",
    "        feature_bools.append(1*(word in free_text))\n",
    "        \n",
    "    return feature_bools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fd0ce93-37de-4312-9646-c94191aad63b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "Analysis Finished\n",
      "When all words are extracted, we have got a dataset of 705858 words\n"
     ]
    }
   ],
   "source": [
    "path = \"dcp-data/letters/\"\n",
    "files = os.listdir(path)\n",
    "files = files[1:]\n",
    "words = []\n",
    "# print(files[1])\n",
    "i = 0\n",
    "cap = 0\n",
    "\n",
    "if cap == 0:\n",
    "    cap = len(files)\n",
    "    \n",
    "data_set = []\n",
    "for file_target in files:\n",
    "    dict_test = extract_info(path+file_target,words)\n",
    "    if dict_test != \"Not Darwin Writing\":\n",
    "        data_set.append(dict_test)\n",
    "#     print(dict_test)\n",
    "    if i == cap:\n",
    "        break\n",
    "    elif i < cap:\n",
    "        i += 1\n",
    "    else:\n",
    "        print(\"Failed loop\")\n",
    "        break\n",
    "    print(round((i/cap)*100,2),end=\"\\r\"*(i!=cap))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Analysis Finished\")\n",
    "print(f\"When all words are extracted, we have got a dataset of {len(words)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "435f83f9-0730-4e4c-996a-540786836506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if \"dump.txt\" not in os.listdir(\".\"):\n",
    "    unique_words = {}\n",
    "    for counter, word in enumerate(words):\n",
    "        try:\n",
    "            unique_words[word] += 1\n",
    "        except KeyError:\n",
    "            unique_words[word] = 1\n",
    "        print(round(((counter+1)/len(words))*100,2),end=\"\\r\")\n",
    "\n",
    "    sorted_unique_words = {key: value for key, value in sorted(unique_words.items(), key=lambda item: item[1],reverse=True)}\n",
    "\n",
    "    print(list(sorted_unique_words.keys())[:1000])\n",
    "    feature_words = list(sorted_unique_words.keys())[:4000]\n",
    "    with open(\"dump.txt\",\"w\",encoding=\"utf8\") as output:\n",
    "        for word in feature_words:\n",
    "            try:\n",
    "                output.write(word +\"\\n\")\n",
    "            except:\n",
    "                print(word)\n",
    "\n",
    "    output.close()\n",
    "\n",
    "with open('dump.txt', encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "lines_cleaned = []\n",
    "for word in lines:\n",
    "    word = word[:-1]\n",
    "    lines_cleaned.append(word)\n",
    "    \n",
    "feature_words = lines_cleaned\n",
    "# print(sum(list(sorted_unique_words.values())[:4000]))\n",
    "# print(sum(list(sorted_unique_words.values())[4000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "03d38b16-8d6c-4a28-a3b1-2564e0f8ac4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7510\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "7861\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "6538\n",
      "6573\n",
      "9999\n",
      "9999\n",
      "6573\n",
      "9999\n",
      "9999\n",
      "7475\n",
      "7475\n",
      "9999\n",
      "7475\n",
      "7475\n",
      "9999\n",
      "9999\n",
      "7475\n",
      "7831\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "7803\n",
      "5454\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "3570\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "7831\n",
      "2134\n",
      "7299\n",
      "7299\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "926\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "7868\n",
      "7868\n",
      "7868\n",
      "7868\n",
      "7868\n",
      "7868\n",
      "{'unique_id': 'DCP-LETT-2115F', 'sender': 'Darwin, C. R.', 'sender_bio': '../nameregs/nameregs_1.xml', 'body': '– july\\xa01857—'}\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "1202\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "3742\n",
      "2581\n",
      "6393\n",
      "6424\n",
      "5882\n",
      "9999\n",
      "9999\n",
      "5882\n",
      "5882\n",
      "9999\n",
      "7223\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "5813\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "6054\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "64\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "7373\n",
      "6660\n",
      "9999\n",
      "5362\n",
      "9999\n",
      "6625\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "6328\n",
      "9999\n",
      "5507\n",
      "9999\n",
      "6142\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "5670\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "7361\n",
      "7232\n",
      "9999\n",
      "7353\n",
      "5854\n",
      "7234\n",
      "9999\n",
      "5662\n",
      "5768\n",
      "9999\n",
      "2656\n",
      "5727\n",
      "7385\n",
      "7385\n",
      "9999\n",
      "5553\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "7475\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "9999\n"
     ]
    }
   ],
   "source": [
    "test = generate_feature_data(data_set[0][\"body\"],feature_words)\n",
    "generate_feature_data(\"I am a woman, ducks are nice\",feature_words)\n",
    "complete_data = []\n",
    "targets = []\n",
    "for dictionary in data_set:\n",
    "    free_text = dictionary[\"body\"]\n",
    "    try:\n",
    "        reciever_id = dictionary[\"reciever_bio\"]\n",
    "    except:\n",
    "        print(dictionary)\n",
    "        continue\n",
    "    number_key = reciever_id[21:-4]\n",
    "    \n",
    "    boolean_set = generate_feature_data(free_text,feature_words)\n",
    "    try:\n",
    "        gender = df[df[\"id\"]==\"DCP-IDENT-\"+str(number_key)][\"sex\"].iloc[0]\n",
    "        if gender == \"\":\n",
    "            gender = \"NotAvailable\"\n",
    "#             print(number_key)\n",
    "    except:\n",
    "        continue\n",
    "#         gender = \"NotAvailable\"\n",
    "#         print(number_key)\n",
    "#         print(reciever_id)\n",
    "#         print(df[df[\"id\"]==\"DCP-IDENT-\"+str(number_key)])\n",
    "        \n",
    "    complete_data.append(boolean_set)\n",
    "    targets.append(gender)\n",
    "#     if counter == 10:\n",
    "#         break\n",
    "#     else:\n",
    "#         counter += 1\n",
    "# print(dict_test[\"sender\"])\n",
    "# print(dict_test[\"reciever\"])\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f14fb45d-5371-4d9c-860f-827720b5083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 7277, 'NotAvailable': 306, '': 225, 'F': 324}\n",
      "0.8948598130841121\n"
     ]
    }
   ],
   "source": [
    "unique_tags = {}\n",
    "for val in targets:\n",
    "    try:\n",
    "        unique_tags[val] += 1\n",
    "    except:\n",
    "        unique_tags[val] = 1\n",
    "        \n",
    "print(unique_tags)\n",
    "print(list(unique_tags.values())[0]/sum(unique_tags.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a01ce938-cb3e-4efd-8bb3-c7771599ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    complete_data, targets, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3ed2b2d2-d1a2-4025-8f0f-4d44356134e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8813767670559312\n"
     ]
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(30)\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9b68c037-e8d3-4389-870c-efe09ffbada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(max_depth=10)\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f54abc86-68d4-4323-a825-6fad9aad79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = classifier.predict(X_test)\n",
    "classifier.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "afd31932-8b65-4e41-9fb3-046700fb7299",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M NotAvailable 0\n",
      "M F 1\n",
      "F M 20\n",
      "M F 23\n",
      "M  24\n",
      "M NotAvailable 47\n",
      "M  70\n",
      "M F 90\n",
      "M  94\n",
      "M NotAvailable 110\n",
      "M NotAvailable 111\n",
      "M NotAvailable 120\n",
      "M NotAvailable 124\n",
      "F M 156\n",
      "M NotAvailable 159\n",
      "M NotAvailable 164\n",
      "M F 167\n",
      "M F 173\n",
      "M F 178\n",
      "M  182\n",
      "M NotAvailable 200\n",
      "M NotAvailable 209\n",
      "M F 220\n",
      "M F 233\n",
      "NotAvailable M 237\n",
      "M NotAvailable 241\n",
      "M NotAvailable 253\n",
      "M F 257\n",
      "M NotAvailable 263\n",
      "M F 275\n",
      "M F 276\n",
      "M NotAvailable 280\n",
      "M NotAvailable 284\n",
      "M  287\n",
      "M NotAvailable 288\n",
      "M  316\n",
      "M F 317\n",
      "M  318\n",
      "M NotAvailable 324\n",
      "M NotAvailable 328\n",
      "M  372\n",
      "M F 382\n",
      "M F 395\n",
      "M F 406\n",
      "M NotAvailable 415\n",
      "M F 418\n",
      "F M 434\n",
      "M F 449\n",
      "M NotAvailable 454\n",
      "M  463\n",
      "M  468\n",
      "M  477\n",
      "M NotAvailable 481\n",
      " M 512\n",
      "M NotAvailable 529\n",
      "M NotAvailable 533\n",
      "M F 536\n",
      "M  537\n",
      "M NotAvailable 540\n",
      "M F 546\n",
      "M  547\n",
      "M F 554\n",
      "M F 561\n",
      "M NotAvailable 575\n",
      "M NotAvailable 577\n",
      "M NotAvailable 584\n",
      "M  585\n",
      "M F 594\n",
      "M NotAvailable 595\n",
      "M F 601\n",
      "M F 604\n",
      "F M 615\n",
      "M F 616\n",
      "F M 621\n",
      "M F 640\n",
      "M NotAvailable 654\n",
      "F  662\n",
      "M  680\n",
      "M F 681\n",
      "M  684\n",
      "M F 690\n",
      "M F 694\n",
      "M  717\n",
      "M NotAvailable 719\n",
      "M  721\n",
      "M F 722\n",
      "F  725\n",
      "M NotAvailable 727\n",
      "M NotAvailable 743\n",
      "M F 745\n",
      "M F 755\n",
      "M NotAvailable 761\n",
      "M NotAvailable 763\n",
      "M F 765\n",
      "M NotAvailable 766\n",
      "NotAvailable M 767\n",
      "M  770\n",
      "M  782\n",
      "M F 784\n",
      "M  791\n",
      "M NotAvailable 798\n",
      "M NotAvailable 801\n",
      "NotAvailable M 806\n",
      "M NotAvailable 813\n",
      "M  814\n",
      "F M 819\n",
      "M F 821\n",
      "M F 830\n",
      "M  850\n",
      "M F 857\n",
      "M F 861\n",
      "M F 866\n",
      "M  873\n",
      "M F 878\n",
      "NotAvailable F 891\n",
      "M  897\n",
      "F M 905\n",
      "M F 906\n",
      "M NotAvailable 907\n",
      "NotAvailable M 915\n",
      "M NotAvailable 920\n",
      "M NotAvailable 934\n",
      "M  937\n",
      "M F 944\n",
      "NotAvailable M 951\n",
      "M  953\n",
      "M NotAvailable 955\n",
      "M F 963\n",
      "F NotAvailable 975\n",
      "M F 985\n",
      " M 989\n",
      "M  990\n",
      "M NotAvailable 999\n",
      "M  1010\n",
      "M  1020\n",
      "M F 1049\n",
      "M  1054\n",
      "M F 1063\n",
      "M F 1074\n",
      "NotAvailable M 1081\n",
      "F M 1086\n",
      "M NotAvailable 1110\n",
      "M  1114\n",
      "M  1116\n",
      "NotAvailable M 1130\n",
      "M F 1134\n",
      "M F 1165\n",
      "M NotAvailable 1177\n",
      "F M 1194\n",
      "M NotAvailable 1195\n",
      "M F 1205\n",
      "M  1220\n",
      "M F 1237\n",
      "M NotAvailable 1243\n",
      "M  1260\n",
      "M NotAvailable 1271\n",
      "M  1291\n",
      "M NotAvailable 1294\n",
      "M  1298\n",
      "M NotAvailable 1314\n",
      "M  1316\n",
      "M  1332\n",
      "M NotAvailable 1334\n",
      "NotAvailable F 1337\n",
      "F M 1341\n",
      "M NotAvailable 1353\n",
      "M F 1355\n",
      "M F 1374\n",
      "NotAvailable F 1377\n",
      "M NotAvailable 1385\n",
      "M NotAvailable 1400\n",
      "M F 1424\n",
      "M  1427\n",
      "M NotAvailable 1428\n",
      "M F 1446\n",
      "M NotAvailable 1460\n",
      "M F 1463\n",
      "M  1464\n",
      "M F 1465\n",
      "M  1474\n",
      "M NotAvailable 1477\n",
      "M NotAvailable 1484\n",
      "M F 1510\n",
      "M  1515\n",
      "M NotAvailable 1529\n",
      "M NotAvailable 1534\n",
      "M F 1545\n",
      "M F 1548\n",
      "M F 1557\n",
      "M F 1566\n",
      "M F 1570\n",
      "M F 1576\n",
      "M  1581\n",
      "M NotAvailable 1595\n",
      "F M 1618\n",
      "M  1619\n",
      " F 1622\n",
      "M F 1623\n"
     ]
    }
   ],
   "source": [
    "for i,val in enumerate(out):\n",
    "    if val != y_test[i]:\n",
    "        print(val,y_test[i],i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4fdbe76f-9c28-4d2d-889a-894faf1e5d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you 0.06098600433356366\n",
      "if 0.010962624869056447\n",
      "was 0.016005989277393818\n",
      "’ 0.013483871100532006\n",
      "many 0.024996687288673648\n",
      "see 0.005695353497336623\n",
      "work 0.010125072884153988\n",
      "c 0.012492934437014925\n",
      "first 0.012204328922864193\n",
      "far 0.01044148141178381\n",
      "kindness 0.024550558456631028\n",
      "better 0.005695353497336623\n",
      "written 0.01079304644248359\n",
      "nothing 0.00683442419680395\n",
      "receive 0.01704559944300416\n",
      "says 0.023555295765375895\n",
      "father 0.05699173113100981\n",
      "pay 0.003796902331557749\n",
      "further 0.008284150541580543\n",
      "d 0.019194508536196864\n",
      "st 0.015946989792542547\n",
      "plan 0.019102427879265683\n",
      "land 0.008775063166266799\n",
      "children 0.01451209197348006\n",
      "set 0.012656341105192497\n",
      "service 0.013400831758439112\n",
      "movement 0.00965915263467713\n",
      "went 0.01066818024192472\n",
      "erasmus 0.021549603277382638\n",
      "black 0.00721411442995972\n",
      "garden 0.029365747714999346\n",
      "admiration 0.01668597873183174\n",
      "captain 0.00664457908022606\n",
      "previous 0.018408057400681674\n",
      "human 0.014789267135696275\n",
      "monograph 0.018280371453393736\n",
      "distance 0.01891513962962701\n",
      "recommend 0.027522231383538886\n",
      "correspondent 0.042013787270139\n",
      "readers 0.02090489925319678\n",
      "equal 0.014488497150896555\n",
      "bot 0.009940980649896647\n",
      "ld 0.005695353497336623\n",
      "plain 0.006508975425527571\n",
      "ps 0.010935422325957048\n",
      "litchfield 0.014133706236423045\n",
      "18 0.02070888964153659\n",
      "minds 0.005062536442076998\n",
      "ducks 0.0136688483936079\n",
      "k 0.013571202546621614\n",
      "varied 0.005062536442076998\n",
      "200 0.015694732470534847\n",
      "un 0.010383952588578391\n",
      "pleasanter 0.012237304501426084\n",
      "ladyship 0.06590809676612538\n",
      "che 0.01604603941858318\n",
      "retained 0.021472584744831543\n",
      "et 0.010697938849967987\n",
      "fanny 0.0149095786561299\n",
      "surgeon 0.011722049575050285\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# print(len(classifier.feature_importances_))\n",
    "\n",
    "test_words = []\n",
    "counter = 0\n",
    "for i,val in enumerate(classifier.feature_importances_):\n",
    "    if val != 0:\n",
    "        counter += 1\n",
    "        print(feature_words[i],val)\n",
    "        \n",
    "print(counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

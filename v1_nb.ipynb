{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3854ecc5-a9f9-4f38-afe6-8840f4335bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81183ef0-ff74-4d90-adfa-ad1b117da3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(filepath):\n",
    "    dictionary_of_interest = {}\n",
    "\n",
    "    with open(filepath,\"r\",encoding=\"utf8\") as file:\n",
    "            \n",
    "        content = file.readlines()\n",
    "        content = \"\".join(content)\n",
    "        \n",
    "        bs_content = bs(content, \"lxml\")\n",
    "\n",
    "        unique_id = bs_content.find(\"tei\").attrs[\"xml:id\"]\n",
    "\n",
    "        letter_details = bs_content.find_all(\"correspaction\")\n",
    "\n",
    "        for deets in letter_details:\n",
    "\n",
    "            if deets.attrs[\"type\"] == \"sent\":\n",
    "\n",
    "                try:\n",
    "                    dictionary_of_interest[\"unique_id\"] = unique_id\n",
    "                    dictionary_of_interest[\"sender\"] = deets.persname.text\n",
    "\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.orgname.text\n",
    "\n",
    "                if \"when\" in list(deets.date.attrs.keys()):\n",
    "                    dictionary_of_interest[\"date\"] = deets.date.attrs[\"when\"]\n",
    "\n",
    "\n",
    "                try:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = deets.persname.attrs[\"key\"]\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = \"None Available\"\n",
    "                except KeyError:\n",
    "                    dictionary_of_interest[\"sender_bio\"] = \"None Available\"\n",
    "                    \n",
    "            if deets.attrs[\"type\"] == \"received\":\n",
    "                \n",
    "                try:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.persname.text\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever\"] = deets.orgname.text\n",
    "                    \n",
    "                try:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = deets.persname.attrs[\"key\"]\n",
    "                except AttributeError:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = \"None Available\"\n",
    "                except KeyError:\n",
    "                    dictionary_of_interest[\"reciever_bio\"] = \"None Available\"\n",
    "                    \n",
    "        try:\n",
    "            free_text = bs_content.find_all(\"div\",{\"type\":\"transcription\"})[0].p.text\n",
    "        except AttributeError:\n",
    "#             print(bs_content) \n",
    "            free_text = \"\"\n",
    "\n",
    "        # cleaning of the data\n",
    "        free_text = free_text.lower().translate(str.maketrans('','',string.punctuation))\n",
    "        go_away_chars = ['’', '“', '‘', '〈', '〉', '–', '♂', '…', '♀', '〈', '〉', '☿', '§', '⊙', '▵', '∴', '„', '✓']\n",
    "        for char in go_away_chars:\n",
    "            free_text = str.replace(free_text, char, \"_\")\n",
    "        dictionary_of_interest[\"body\"] = free_text\n",
    "\n",
    "        file.close()\n",
    "\n",
    "    return dictionary_of_interest\n",
    "\n",
    "def generate_feature_data(free_text,feature_set):\n",
    "    \n",
    "    feature_bools = []\n",
    "    \n",
    "    for word in feature_set:\n",
    "        feature_bools.append(1*(word in free_text))\n",
    "        \n",
    "    return feature_bools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd0ce93-37de-4312-9646-c94191aad63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "Analysis Finished\n",
      "When all words are extracted, we have got a dataset of 593445 words in letters TO Darwin\n",
      "When all words are extracted, we have got a dataset of 695012 words in letters FROM Darwin\n"
     ]
    }
   ],
   "source": [
    "path = \"dcp-data/letters/\"\n",
    "files = os.listdir(path)\n",
    "# to be commented out depending on who is running the code (lol) \n",
    "files = files[1:]\n",
    "files = files[:len(files)-1]\n",
    "words_darwin = []\n",
    "words_not_darwin = []\n",
    "\n",
    "i = 0\n",
    "cap = 0\n",
    "\n",
    "if cap == 0:\n",
    "    cap = len(files)\n",
    "\n",
    "for file_target in files:\n",
    "    dict_cur = extract_info(path+file_target)\n",
    "    cur_words = words_not_darwin\n",
    "    if \"sender\" in dict_cur.keys() and dict_cur[\"sender\"] == \"Darwin, C. R.\":\n",
    "        cur_words = words_darwin\n",
    "    text_tokens = word_tokenize(dict_cur[\"body\"])\n",
    "\n",
    "    for word in text_tokens:\n",
    "        if len(word) == 1 and not(word in [\"i\",\"a\"]):\n",
    "            continue\n",
    "        else:\n",
    "            cur_words.append(word)\n",
    "\n",
    "    if i == cap:\n",
    "        break\n",
    "    elif i < cap:\n",
    "        i += 1\n",
    "    else:\n",
    "        print(\"Failed loop\")\n",
    "        break\n",
    "    print(round((i/cap)*100,2),end=\"\\r\"*(i!=cap))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Analysis Finished\")\n",
    "print(f\"When all words are extracted, we have got a dataset of {len(words_not_darwin)} words in letters TO Darwin\")\n",
    "print(f\"When all words are extracted, we have got a dataset of {len(words_darwin)} words in letters FROM Darwin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2cf6e6-0718-442e-b657-921b5463df7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695012\n",
      "\n",
      "593445\n",
      "\n",
      "100.0\r"
     ]
    }
   ],
   "source": [
    "unique_both = {\"darwin\" : words_darwin, \"not_darwin\" : words_not_darwin}\n",
    "for key in unique_both.keys():\n",
    "    unique_words = {}\n",
    "    words = unique_both[key]\n",
    "    print(len(words))\n",
    "    print(\"\")\n",
    "    for counter, word in enumerate(words):\n",
    "        try:\n",
    "            unique_words[word] += 1\n",
    "        except KeyError:\n",
    "            unique_words[word] = 1\n",
    "        print(round(((counter+1)/len(words))*100,2),end=\"\\r\")\n",
    "\n",
    "    sorted_unique_words = {key: value for key, value in sorted(unique_words.items(), key=lambda item: item[1],reverse=True)}\n",
    "\n",
    "#     print(list(sorted_unique_words.keys())[:1000])\n",
    "\n",
    "    feature_words = list(sorted_unique_words.keys())[:4000]\n",
    "\n",
    "    with open(\"dump_\" + key + \".txt\",\"w\",encoding=\"utf8\") as output:\n",
    "        for word in feature_words:\n",
    "            try:\n",
    "                output.write(word +\"\\n\")\n",
    "            except:\n",
    "                print(word)\n",
    "\n",
    "    output.close()\n",
    "    unique_both[key] = feature_words\n",
    "# print(dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435f83f9-0730-4e4c-996a-540786836506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be opened for both files\n",
    "filenames = {\"darwin\": \"dump_darwin.txt\", \"not_darwin\": \"dump_not_darwin.txt\"}\n",
    "feature_words_all = []\n",
    "for key in filenames.keys():\n",
    "    file = filenames[key]\n",
    "    with open(file, \"w\", encoding=\"utf8\") as output:\n",
    "        for word in unique_both[key]:\n",
    "            try:\n",
    "                output.write(word +\"\\n\")\n",
    "            except:\n",
    "                print(word)\n",
    "        \n",
    "    output.close()\n",
    "\n",
    "    with open(file, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    lines_cleaned = []\n",
    "    for word in lines:\n",
    "        word = word[:-1]\n",
    "        lines_cleaned.append(word)\n",
    "    \n",
    "    feature_words = lines_cleaned\n",
    "    feature_words_all.append(feature_words)\n",
    "\n",
    "# print(sum(list(sorted_unique_words.values())[:4000]))\n",
    "# print(sum(list(sorted_unique_words.values())[4000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03d38b16-8d6c-4a28-a3b1-2564e0f8ac4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# should check the sender before choosing which feature_words to use (feature_words_all[0] = darwin, feature_words_all[1] = not darwin)\n",
    "#test = generate_feature_data(dict_test[\"body\"],feature_words)\n",
    "#print(dict_test[\"sender\"])\n",
    "#print(dict_test[\"reciever\"])\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14fb45d-5371-4d9c-860f-827720b5083d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}